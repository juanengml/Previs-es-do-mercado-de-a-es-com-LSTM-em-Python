{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnaliseExploratoria-CANDLE DOLAR 5 MIN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nZhr-eFYj-f",
        "colab_type": "text"
      },
      "source": [
        "#Analisando candles do dolar de 5 minutos\n",
        "## usando RNN e MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvqewV_k_EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.6.0\n",
        "!pip show tensorflow  # 1.6.0\n",
        "!pip show keras  # 2.1.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRn6zgvCWyuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import urllib.request, json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf # TensorFlow 1.6\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PASLZ8IEXJcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/juanengml/Previs-es-do-mercado-de-a-es-com-LSTM-em-Python/master/CANDLE%20DOLAR%205%20MIN.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgG6il9wXeEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1r9VPeQXhaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"CANDLE DOLAR 5 MIN.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-D4XZpEX-4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.sort_values('dt_registro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwI1R5DKn3_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2e0w65qYdl2",
        "colab_type": "text"
      },
      "source": [
        "## Nesse processo vamos usar as colunas low e high"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ya5e0BWX_3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),(df['low']+df['high'])/2.0)\n",
        "plt.xticks(range(0,df.shape[0],500),df['dt_registro'].loc[::500],rotation=45)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8An4DG-4Y0Ru",
        "colab_type": "text"
      },
      "source": [
        "##Definindo preços altos, baixos e media "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djduDgJYYbTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preco_alto = df.loc[:,'high'].as_matrix()\n",
        "preco_baixo = df.loc[:,'low'].as_matrix()\n",
        "media_precos = (preco_alto+preco_baixo)/2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9gcwzIQjpKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(media_precos)/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GKUmfXKZCNh",
        "colab_type": "text"
      },
      "source": [
        "## Splinting dados de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cul9_ysYq9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_treino = media_precos[:11000]\n",
        "data_teste = media_precos[11000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_aInUJZLqt",
        "colab_type": "text"
      },
      "source": [
        "## O que é MinMaxScaller ? \n",
        "Para cada valor em um recurso, o MinMaxScaler subtrai o valor mínimo no recurso e depois divide pelo intervalo. O intervalo é a diferença entre o máximo original e o mínimo original.\n",
        "\n",
        "MinMaxScaler preserva a forma da distribuição original. Não altera significativamente as informações incorporadas nos dados originais. Observe que o MinMaxScaler não reduz a importância dos valores discrepantes.O intervalo padrão para o recurso retornado pelo MinMaxScaler é de 0 a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB5JUqV_Yskh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "data_treino = data_treino.reshape(-1,1)\n",
        "data_teste = data_teste.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbGyHiIKZxZg",
        "colab_type": "text"
      },
      "source": [
        "#### Normalizando o último bit de dados restante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzuHBgAqYwif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tamano_da_janela_ = 2500\n",
        "for di in range(0,10000,tamano_da_janela_):\n",
        "    scaler.fit(data_treino[di:di+tamano_da_janela_,:])\n",
        "    data_treino[di:di+tamano_da_janela_,:] = scaler.transform(data_treino[di:di+tamano_da_janela_,:])\n",
        "\n",
        "scaler.fit(data_treino[di+tamano_da_janela_:,:])\n",
        "data_treino[di+tamano_da_janela_:,:] = scaler.transform(data_treino[di+tamano_da_janela_:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yifkwJddZ6kL",
        "colab_type": "text"
      },
      "source": [
        "### Normalizar dados de teste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfFLWXZAYuUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_treino = data_treino.reshape(-1)\n",
        "\n",
        "#  Normalizar dados de teste\n",
        "data_teste = scaler.transform(data_teste).reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79-EdsjYzNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMA = 0.0\n",
        "gamma = 0.1\n",
        "for ti in range(11000):\n",
        "  EMA = gamma*data_treino[ti] + (1-gamma)*EMA\n",
        "  data_treino[ti] = EMA\n",
        "\n",
        "all_mid_data = np.concatenate([data_treino,data_teste],axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL5ZdRG6astI",
        "colab_type": "text"
      },
      "source": [
        "## Plot "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIhzGIcrY4M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "janela = 100\n",
        "N = data_treino.size\n",
        "std_avg_predictions = []\n",
        "std_avg_x = []\n",
        "mse_errors = []\n",
        "for pred_idx in range(janela,N):\n",
        "    if pred_idx >= N:\n",
        "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
        "    else:\n",
        "        date = df.loc[pred_idx,'dt_registro']\n",
        "    std_avg_predictions.append(np.mean(data_treino[pred_idx-janela:pred_idx]))\n",
        "    mse_errors.append((std_avg_predictions[-1]-data_treino[pred_idx])**2)\n",
        "    std_avg_x.append(date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMvQQHn2Y15D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
        "plt.plot(range(janela,N),std_avg_predictions,color='orange',label='Prediction')\n",
        "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Mid Price')\n",
        "plt.legend(fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ZXkoWVY-MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 100\n",
        "N = data_treino.size\n",
        "run_avg_predictions = []\n",
        "run_avg_x = []\n",
        "mse_errors = []\n",
        "running_mean = 0.0\n",
        "run_avg_predictions.append(running_mean)\n",
        "decay = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucQvc_yLa9VQ",
        "colab_type": "text"
      },
      "source": [
        "## Tafuq MSE ? \n",
        "O erro médio quadrático (MSE) de um estimador mede a média dos quadrados dos erros,isto é, a diferença quadrática média entre os valores estimados e o valor real. É uma função de risco, correspondente ao valor esperado da perda de erro ao quadrado. É sempre não negativo e valores próximos de zero são melhores. O MSE é o segundo momento do erro (sobre a origem) e, portanto, incorpora a variação do estimador e seu viés.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUZH7zeGd4cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for pred_idx in range(1,N):\n",
        "    running_mean = running_mean*decay + (1.0-decay)*data_treino[pred_idx-1]\n",
        "    run_avg_predictions.append(running_mean)\n",
        "    mse_errors.append((run_avg_predictions[-1]-data_treino[pred_idx])**2)\n",
        "    run_avg_x.append(date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlmVKhPRd7sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('MSE error - EMA: %.5f'%(0.5*np.mean(mse_errors)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hgh4Ww9d9nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (18,9))\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
        "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
        "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Mid Price')\n",
        "plt.legend(fontsize=18)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg0jOhODltPI",
        "colab_type": "text"
      },
      "source": [
        "# Usando LSTM para Stock Prediciton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Qo6MDIlUsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGeneratorSeq(object):\n",
        "\n",
        "    def __init__(self,prices,batch_size,num_unroll):\n",
        "        self._prices = prices\n",
        "        self._prices_length = len(self._prices) - num_unroll\n",
        "        self._batch_size = batch_size\n",
        "        self._num_unroll = num_unroll\n",
        "        self._segments = self._prices_length //self._batch_size\n",
        "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
        "\n",
        "    def next_batch(self):\n",
        "\n",
        "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
        "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
        "\n",
        "        for b in range(self._batch_size):\n",
        "            if self._cursor[b]+1>=self._prices_length:\n",
        "                #self._cursor[b] = b * self._segments\n",
        "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
        "\n",
        "            batch_data[b] = self._prices[self._cursor[b]]\n",
        "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
        "\n",
        "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
        "\n",
        "        return batch_data,batch_labels\n",
        "\n",
        "    def unroll_batches(self):\n",
        "\n",
        "        unroll_data,unroll_labels = [],[]\n",
        "        init_data, init_label = None,None\n",
        "        for ui in range(self._num_unroll):\n",
        "\n",
        "            data, labels = self.next_batch()    \n",
        "\n",
        "            unroll_data.append(data)\n",
        "            unroll_labels.append(labels)\n",
        "\n",
        "        return unroll_data, unroll_labels\n",
        "\n",
        "    def reset_indices(self):\n",
        "        for b in range(self._batch_size):\n",
        "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqAQaCGAeCCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dg = DataGeneratorSeq(data_treino,5,5)\n",
        "u_data, u_labels = dg.unroll_batches()\n",
        "\n",
        "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
        "    print('\\n\\nUnrolled index %d'%ui)\n",
        "    dat_ind = dat\n",
        "    lbl_ind = lbl\n",
        "    print('\\tInputs: ',dat )\n",
        "    print('\\n\\tOutput:',lbl)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIl3eOOueNDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = 1 # Dimensionalidade dos dados. Como seus dados são 1-D, isso seria 1\n",
        "num_unrollings = 50 # NuEm várias etapas do tempo, você olha para o futuro.\n",
        "batch_size = 500 # Número de amostras em um lote\n",
        "num_nodes = [200,200,150] # Número de nós ocultos em cada camada da pilha profunda do LSTM que estamos usando\n",
        "n_layers = len(num_nodes) # Numero de camadas\n",
        "dropout = 0.2 # dropout\n",
        "\n",
        "tf.reset_default_graph() # Isso é importante caso você execute isso várias vezes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDsEvyhqeVV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# entrada de dados\n",
        "train_inputs, train_outputs = [],[]\n",
        "\n",
        "# Você desenrola a entrada ao longo do tempo, definindo espaços reservados para cada etapa\n",
        "for ui in range(num_unrollings):\n",
        "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
        "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc7X2ypxhNUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_cells = [\n",
        "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
        "         state_is_tuple=True,initializer= tf.contrib.layers.xavier_initializer()\n",
        "                           )\n",
        "         for li in range(n_layers)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6XG9SdSe1Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
        "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
        ") for lstm in lstm_cells]\n",
        "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
        "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
        "\n",
        "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhQnLFIve4W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crie variáveis ​​de estado da célula e estado oculto para manter o estado do LSTM\n",
        "\n",
        "c, h = [],[]\n",
        "initial_state = []\n",
        "for li in range(n_layers):\n",
        "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
        "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
        "  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
        "\n",
        "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
        "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
        "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
        "    time_major = True, dtype=tf.float32)\n",
        "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
        "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
        "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSlLgtU5l5mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ao calcular a perda, você precisa ter cuidado com a forma exata, porque calcula\n",
        "# perda de todas as etapas desenroladas ao mesmo tempo\n",
        "# Portanto, considere o erro médio ou cada lote e obtenha a soma disso em todas as etapas desenroladas\n",
        "\n",
        "print('Perdade Treino')\n",
        "loss = 0.0\n",
        "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
        "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
        "  for ui in range(num_unrollings):\n",
        "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
        "\n",
        "print('Rate de Aprendizado em opecarao decay')\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "inc_gstep = tf.assign(global_step,global_step + 1)\n",
        "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
        "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
        "\n",
        "learning_rate = tf.maximum(\n",
        "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
        "    tf_min_learning_rate)\n",
        "\n",
        "# Otimização.\n",
        "print('TF Otimizacao')\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "optimizer = optimizer.apply_gradients(\n",
        "    zip(gradients, v))\n",
        "\n",
        "print('\\tDahora DOne')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUm0yD0Pl8-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Definindo funcoes TF relacionadas a previsao')\n",
        "\n",
        "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
        "\n",
        "# Manutenção do estado LSTM para o estágio de previsão\n",
        "sample_c, sample_h, initial_sample_state = [],[],[]\n",
        "for li in range(n_layers):\n",
        "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
        "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
        "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x9RmXfvmCvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
        "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
        "\n",
        "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
        "                                   initial_state=tuple(initial_sample_state),\n",
        "                                   time_major = True,\n",
        "                                   dtype=tf.float32)\n",
        "\n",
        "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
        "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
        "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
        "\n",
        "print('\\tDone cacete')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX_dRZxpe2BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzhB1uY8mH9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "valid_summary = 1 # Intervalo em que você faz previsões de teste\n",
        "n_predict_once = 50 # Número de etapas que você prevê continuamente\n",
        "train_seq_length = data_treino.size # Comprimento total dos dados de treinamento\n",
        "train_mse_ot = [] # Acumular perdas de trem\n",
        "test_mse_ot = [] # Acumular perda de teste\n",
        "predictions_over_time = [] # Acumular previsões\n",
        "session = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()\n",
        "# Usado para diminuir a taxa de aprendizado\n",
        "loss_nondecrease_count = 0\n",
        "loss_nondecrease_threshold = 2 # Se o erro de teste não aumentar nessas etapas, diminua a taxa de aprendizado\n",
        "print('Inicio do role.....')\n",
        "average_loss = 0\n",
        "# Definir gerador de dados\n",
        "\n",
        "data_gen = DataGeneratorSeq(data_treino,batch_size,num_unrollings)\n",
        "x_axis_seq = []\n",
        "# Pontos pelos quais você inicia suas previsões de teste\n",
        "test_points_seq = np.arange(11000,12000,50).tolist()\n",
        "for ep in range(epochs):       \n",
        "    # treino\n",
        "    print(datetime.now())\n",
        "    for step in range(train_seq_length//batch_size):\n",
        "        u_data, u_labels = data_gen.unroll_batches()\n",
        "        feed_dict = {}\n",
        "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
        "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
        "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
        "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
        "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "        average_loss += l\n",
        "    # Validação\n",
        "    if (ep+1) % valid_summary == 0:\n",
        "      average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
        "      if (ep+1)%valid_summary==0:\n",
        "        print('Media de perda step %d: %f' % (ep+1, average_loss))\n",
        "      train_mse_ot.append(average_loss)\n",
        "      average_loss = 0 \n",
        "      predictions_seq = []\n",
        "      mse_test_loss_seq = []\n",
        "      # Atualizacao de Stado e Predictions\n",
        "      for w_i in test_points_seq:\n",
        "        mse_test_loss = 0.0\n",
        "        our_predictions = []\n",
        "\n",
        "        if (ep+1)-valid_summary==0:\n",
        "          x_axis=[]\n",
        "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
        "          current_price = all_mid_data[tr_i]\n",
        "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
        "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
        "\n",
        "        feed_dict = {}\n",
        "        current_price = all_mid_data[w_i-1]\n",
        "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
        "        for pred_i in range(n_predict_once):\n",
        "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
        "          our_predictions.append(np.asscalar(pred))\n",
        "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
        "          if (ep+1)-valid_summary==0:\n",
        "            x_axis.append(w_i+pred_i)\n",
        "          mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
        "          \n",
        "        session.run(reset_sample_states)\n",
        "        predictions_seq.append(np.array(our_predictions))\n",
        "        mse_test_loss /= n_predict_once\n",
        "        mse_test_loss_seq.append(mse_test_loss)\n",
        "        if (ep+1)-valid_summary==0:\n",
        "          x_axis_seq.append(x_axis)\n",
        "      current_test_mse = np.mean(mse_test_loss_seq)\n",
        "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
        "          loss_nondecrease_count += 1\n",
        "      else:\n",
        "          loss_nondecrease_count = 0\n",
        "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
        "            session.run(inc_gstep)\n",
        "            loss_nondecrease_count = 0\n",
        "            print('\\tlearning rate by 0.5')\n",
        "      test_mse_ot.append(current_test_mse)\n",
        "      print('\\tTeste MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
        "      predictions_over_time.append(predictions_seq)\n",
        "      print('\\tFim cacete de Predictions',datetime.now())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGBthKm8mgSe",
        "colab_type": "text"
      },
      "source": [
        "# Visualização da Precisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBdgVUumPsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_prediction_epoch = 20 # substitua isso pela época em que você obteve os melhores resultados ao executar o código de plotagem\n",
        "\n",
        "plt.figure(figsize = (18,18))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
        "\n",
        "# Traçar como as previsões mudam ao longo do tempo\n",
        "# Plotar previsões mais antigas com alfa baixo e previsões mais recentes com alfa alto\n",
        "\n",
        "start_alpha = 0.25\n",
        "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
        "for p_i,p in enumerate(predictions_over_time[::3]):\n",
        "    for xval,yval in zip(x_axis_seq,p):\n",
        "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
        "\n",
        "plt.title('Evolution of Test Predictions Over Time',fontsize=18)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.xlim(11000,12500)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "\n",
        "# Como prever a melhor previsão de teste que você obteve\n",
        "\n",
        "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
        "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n",
        "    plt.plot(xval,yval,color='r')\n",
        "\n",
        "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
        "plt.xlabel('Date',fontsize=18)\n",
        "plt.ylabel('Mid Price',fontsize=18)\n",
        "plt.xlim(11000,12500)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqU6rp5KbbEL",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# FONTE DE PESQUISA\n",
        "\n",
        "MinMaxScale - https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
        "\n",
        "MSE - https://www.geeksforgeeks.org/python-mean-squared-error/\n",
        "\n",
        "LSTM - http://deeplearningbook.com.br/arquitetura-de-redes-neurais-long-short-term-memory/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQgclLa2btAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}